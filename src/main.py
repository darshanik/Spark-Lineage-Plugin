import datetime
import os
import json
import logging
import time
from dotenv import load_dotenv
from flask import Flask, request, jsonify
from flask_restful import Api, Resource
from logging.handlers import TimedRotatingFileHandler
from confluent_kafka import Consumer, TopicPartition
from entities.table import Table
from entities.container import Container
from entities.lineage import Lineage
from entities.utils import deserialize, map_entity_type, get_env_var
load_dotenv("C:\\Users\\Darshanik\\OneDrive - University of Texas at Arlington\\Desktop\\Spark-Lineage-Transform\\src\\.env")
LOG_PATH = get_env_var("SPARK_LINEAGE_PLUGIN_LOG_PATH")
LOG_INTERVAL = int(get_env_var("SPARK_LINEAGE_PLUGIN_LOG_INTERVAL"))
LOG_BACKUP_COUNT = int(get_env_var("SPARK_LINEAGE_PLUGIN_LOG_BACKUP_COUNT"))
LOG_INTERVAL_TYPE = get_env_var("SPARK_LINEAGE_PLUGIN_LOG_INTERVAL_TYPE")
LOG_LEVEL = get_env_var("SPARK_LINEAGE_PLUGIN_LOG_LEVEL")

logging.basicConfig(
    format="%(asctime)s %(message)s",
    handlers=[
        TimedRotatingFileHandler(
            LOG_PATH,
            when=LOG_INTERVAL_TYPE,
            interval=LOG_INTERVAL,
            backupCount=LOG_BACKUP_COUNT,
            encoding="utf-8",
            delay=False,
        ),
        logging.StreamHandler(),
    ],
    level=LOG_LEVEL,
)
# Defining and Setting Logger to keep track of progress and events of app!
logging.basicConfig(
    format="%(asctime)s %(message)s",
    handlers=[logging.FileHandler(LOG_PATH), logging.StreamHandler()],
)
logger = logging.getLogger()
'''
This method will parse through the entire facet generated by open lineage spark listener
Flow of parsing:
-> transform method will take in the event message emitted by spark listener,

-> It will filter out the messages that are not Complete or running, as the other messages
does not have inputs or outputs facets or has the facets which are empty.

-> If the input satisfies above conditions, the method will call the corresponding mapper key 
and initiates the class that is associated with the mapper key by passing "items" from input facets.

-> Then it appends the object "input_entity" to a list, making it a list of objects which will later 
be used by lineage class to construct the JSON payload to deliver it to openmetadata Rest API.

Note: its at this point, the actual lineage will be generated in openmetadata.

->Next, once it passes through spark process and sends payload to openmetadata, it will look for
a facet called "columnLineage" in the event message. If it finds it, it will invoke a class called
SparkColumnLineage and the resulting object will send column lineage payload to  
'''
def transform(facet):
    mapper = {
        "Container": Container,
        "Table": Table,
        "Lineage": Lineage
    }

    if ("eventType" in facet and facet['eventType'] == "COMPLETE") or ("eventType" in facet and facet['eventType'] == "RUNNING"):
        USE_KAFKA_CONN = get_env_var("SPARK_LINEAGE_PLUGIN_USE_KAFKA_CONN")
        INTERACT_WITH_OPENMETADATA = get_env_var("SPARK_LINEAGE_PLUGIN_INTERACT_WITH_OPENMETADATA")               
        if "inputs" in facet and len(facet["inputs"]) > 0:
            for items in facet["inputs"]:
                if "facets" in items:
                    mapper_key = map_entity_type(items)
                    input_entity = mapper[mapper_key](items)
                    if INTERACT_WITH_OPENMETADATA == "TRUE":
                        input_entity_id, type_def = input_entity.get_entity_id()
                        logger.info(f"==> input_entity_id extracted from Openmetadata: {input_entity_id}")
                        logger.debug(f"constructed typedef for {mapper_key}: ")
                        logger.debug(type_def)
                        return input_entity_id, type_def
                else:
                    logger.debug(
                        "rejected message for parsing due to missing required key called FACETS in inputs!! "
                    )
                    logger.debug(f"rejected message:{facet}")
        if "outputs" in facet and len(facet["outputs"]) > 0:
            for items in facet["outputs"]:
                if "facets" in items:
                    mapper_key = map_entity_type(items)
                    output_entity = mapper[mapper_key](items)
                    if INTERACT_WITH_OPENMETADATA == "TRUE":
                        output_entity_id, type_def = output_entity.get_entity_id()
                        logger.info(f"==> input_entity_id extracted from Openmetadata: {input_entity_id}")
                        logger.debug(f"constructed typedef for {mapper_key}: ")
                        logger.debug(type_def)                        
                        return output_entity_id, type_def
                else:
                    logger.debug(
                        "rejected message for parsing due to missing required key called FACETS in outputs!! "
                    )
                    logger.debug(f"rejected message:{facet}")
        if len(facet["outputs"]) > 0 and len(facet["inputs"]) > 0:
            if "job" in facet:
                jobName = facet["job"]["name"]
                lineageEntity = Lineage(items)
                sparkLineagePayload = lineageEntity.build_lineage_edge()
        else:
            logger.debug(
                "Null inputs or outputs facets are detected in the message!! Skipping lineage"
            )
    else:
        pass
'''
This method if invoked, 
-> Will accepts a message from kafka topic,

-> Decodes the message from utf-8

-> Deserializes message into list of messages by calling "deserialize method".

Note: In many instances, open lineage spark emitter will emit multiple event messages
not separated by ",". 
"deserialize method will make it more organized for parsing.

-> And iterates over the list of deserialized messages by passing each message
to "transformer" method

-> Additionally, it also logs the KPIs such as message size in Kb,
start time of processing and end time of processing. These metrics are by default
 and will be logged at "info" level. 
 
'''
def kafkaProcessor(message):
    logger.debug("\n Consumer initiated !!")
    try:
        decoded_message = message.value().decode("utf-8")
        logger.debug(
            f"Connected to Topic: {message.topic()} and Partition: {message.partition()}"
        )
        logger.debug(
            f"Received Message: {decoded_message} with Offset: {message.offset()}"
        )
        message_size = len(decoded_message.encode('utf-8'))/1024
        processing_start_time = datetime.datetime.now()
        logger.debug("\n deserializing the message!!")
        deserialized_message = deserialize(decoded_message)
        logger.debug("\n converting")
        deserialized_data_list = json.loads(deserialized_message)
        for facet in deserialized_data_list:
            try:
                transform(facet)
                processing_end_time = datetime.datetime.now()
                elapsed_time = processing_end_time - processing_start_time
                logger.info(f'size={message_size}kb,start={processing_start_time},end={processing_end_time},elapsed={elapsed_time.seconds}')
            except Exception as e:
                logger.error(str(e))
                continue
    except Exception as e:
        logger.error(str(e))

app = Flask(__name__)
api = Api(app)
@app.route('/event_message', methods=['POST'])
def apiProcessor():
    logger.info("==> API Initiated <==")
    if request.method == 'POST':
        spark_event_message = request.get_json()
    logger.info("==>Processing event message")
    ARCHIVE_EVENT_MESSAGES = get_env_var("SPARK_LINEAGE_PLUGIN_ARCHIVE_EVENT_MESSAGES")
    ARCHIVE_EVENT_MESSAGES_DIRECTORY = get_env_var("SPARK_LINEAGE_PLUGIN_ARCHIVE_EVENT_MESSAGES_DIRECTORY")
    message_dump = json.dumps(spark_event_message)
    deserialized_message = deserialize(message_dump)
    deserialized_message_list = json.loads(deserialized_message)
    message_size = len(deserialized_message.encode('utf-8'))/1024
    processing_start_time = datetime.datetime.now()
    for facet in deserialized_message_list:
        try:
            output_entity_id, type_def = transform(facet)
            processing_end_time = datetime.datetime.now()
            elapsed_time = processing_end_time - processing_start_time
            logger.info(f'size={message_size}kb,start={processing_start_time},end={processing_end_time},elapsed={elapsed_time.seconds}')
            if LOG_LEVEL == "DEBUG":
                return type_def
            else:
                return jsonify({"entityId": output_entity_id,
                                "status": "processed"})
        except Exception as e:
            logger.error(str(e))
            return  str(e)
    if ARCHIVE_EVENT_MESSAGES == "TRUE":
        archive_file_directory = ARCHIVE_EVENT_MESSAGES_DIRECTORY
        timestr = time.strftime("%Y-%m-%d-%H:%M%S")
        archive_filename = f"spark_lineage_plugin_{timestr}.txt"
        '''Creates file with timestamp and writes the received event message'''
        with open(os.path.join(archive_file_directory,archive_filename),'a') as fileName:
            fileName.write(json.dumps(spark_event_message, indent=3))
            res = {"status": "success"}
            logger.info(jsonify(res))
'''
Below is the main initiation which has the consumer configs,
and this initiates the above parser method there by triggering entire flow.
'''
if __name__ == "__main__":
    while True:
        READ_FROM_KAFKA = get_env_var("SPARK_LINEAGE_PLUGIN_READ_FROM_KAFKA")
        if READ_FROM_KAFKA == "TRUE":
            consumer_cofig_dict = {
                "bootstrap.servers": get_env_var("KAFKA_BOOTSTRAP_SERVERS"),
                "group.id": get_env_var("KAFKA_GROUP_ID"),
                "security.protocol": get_env_var("KAFKA_SECURITY_PROTOCOL"),
                "auto.offset.reset": get_env_var("KAFKA_AUTO_OFFSET_RESET"),
                "enable.auto.commit": True,
                "heartbeat.interval.ms": 50000,
                "session.timeout.ms": 300000,
                "max.poll.interval.ms": 700000,
                "sasl.mechanisms": get_env_var("KAFKA_SASL_MECHANISMS"),
                "sasl.username": get_env_var("KAFKA_SASL_USERNAME"),
                "sasl.password": get_env_var("KAFKA_SASL_PASSWORD"),
                "ssl.ca.location": get_env_var("KAFKA_SSL_CA_LOCATION"),
            }
            OPENLINEAGE_KAFKA_TOPIC_NAME = get_env_var("OPEN_LINEAGE_KAFKA_TOPIC_NAME")
            consumer = Consumer(consumer_cofig_dict)
            logger.info("consumer configured successfully!")
            consumer.subscribe([OPENLINEAGE_KAFKA_TOPIC_NAME])
            logger.info(
                f"consumer subscribed to topic {OPENLINEAGE_KAFKA_TOPIC_NAME} successfully!"
            )
            message = consumer.poll(1.0)
            current_date = datetime.datetime.now()
            log_time = current_date.strftime("%B %d time- %H : %M : %S")
            if message is not None:
                logger.info(kafkaProcessor(message))
            elif message is None:
                logger.debug(
                    f" no messages detected in topic {OPENLINEAGE_KAFKA_TOPIC_NAME} at {log_time}!"
                )
                logger.debug(" waiting for 1 sec to try again!! ")
                time.sleep(1.0)
                continue
        else:
            current_date = datetime.datetime.now()
            log_time = current_date.strftime("%B %d time- %H : %M : %S")
            app.run(debug=False, host='0.0.0.0', port='8080')
            
