import datetime
import os
import json
import logging
import time
from logging.handlers import TimedRotatingFileHandler
from confluent_kafka import Consumer, TopicPartition
from entities.hive_entity import HiveTable
from entities.rdbms_table import RdbmsTable
from entities.s3_entity import S3V2Directory
from entities.spark_column_lineage import SparkColumnLineage
from entities.spark_entity import SparkProcess
from entities.standard_file import StandardFile
from entities.snowflake_entity import SnowflakeTable
from entities.utils import deserialize, identify_entity_type, retrieve_env_var
LOG_PATH = retrieve_env_var("SPARK_LINEAGE_TRANSFORM_LOG_PATH")
LOG_INTERVAL = int(retrieve_env_var("SPARK_LINEAGE_TRANSFORM_LOG_INTERVAL"))
LOG_BACKUP_COUNT = int(retrieve_env_var("SPARK_LINEAGE_TRANSFORM_LOG_BACKUP_COUNT"))
LOG_INTERVAL_TYPE = retrieve_env_var("SPARK_LINEAGE_TRANSFORM_LOG_INTERVAL_TYPE")
LOG_LEVEL = retrieve_env_var("SPARK_LINEAGE_TRANSFORM_LOG_LEVEL")

logging.basicConfig(
    format="%(asctime)s %(message)s",
    handlers=[
        TimedRotatingFileHandler(
            LOG_PATH,
            when=LOG_INTERVAL_TYPE,
            interval=LOG_INTERVAL,
            backupCount=LOG_BACKUP_COUNT,
            encoding="utf-8",
            delay=False,
        ),
        logging.StreamHandler(),
    ],
    level=LOG_LEVEL,
)
# Defining and Setting Logger to keep track of progress and events of app!
logging.basicConfig(
    format="%(asctime)s %(message)s",
    handlers=[logging.FileHandler(LOG_PATH), logging.StreamHandler()],
)
logger = logging.getLogger()
'''
This method will parse through the entire facet generated by open lineage spark listener
Flow of parsing:
-> transformer method will take in the event message emitted by spark listener,

-> It will filter out the messages that are not Complete or running, as the other messages
does not have inputs or outputs facets or has the facets which are empty.

-> If the input satisfies above conditions, the method will call the corresponding mapper key 
and initiates the class that is associated with the mapper key by passing "items" from input facets.

-> Then it appends the object "atlas_entity" to a list, making it a list of objects which will later 
be used by spark_process class to construct the JSON payload to deliver it to Atlas kafka topic or
Atlas Rest API.

Note: its at this point, the actual lineage will be generated in Atlas.

->Next, once it passes through spark process and sends payload to Atlas, it will look for
a facet called "columnLineage" in the event message. If it finds it, it will invoke a class called
SparkColumnLineage and the resulting object will send column lineage payload to Atlas 

Note: This Transformer is capable to send the lineage payload to Atlas by 2 means:
    1. Through the Kafka topic, Suppose if your atlas installation has kafka topic, this method will emit the payload
    to that topic
    2. Through Atlas REST API, which is default as Atlas comes with the APi
'''
def transform(facet):
    mapper = {
        "S3_v2_directory": S3V2Directory,
        "HiveTable": HiveTable,
        "file": StandardFile,
        "RdbmsTable": RdbmsTable,
        "SnowflakeTable": SnowflakeTable
    }

    if ("eventType" in facet and facet['eventType'] == "COMPLETE") or ("eventType" in facet and facet['eventType'] == "RUNNING"):
        USE_KAFKA_CONN = retrieve_env_var("SPARK_LINEAGE_TRANSFORM_USE_KAFKA_CONN")
        INTERACT_WITH_OPENMETADATA = retrieve_env_var("SPARK_LINEAGE_TRANSFORM_INTERACT_WITH_OPENMETADATA")
        INTERACT_WITH_ATLAS = retrieve_env_var("SPARK_LINEAGE_TRANSFORM_INTERACT_WITH_ATLAS")
        if "inputs" in facet and len(facet["inputs"]) > 0:
            for items in facet["inputs"]:
                if "facets" in items:
                    mapper_key = identify_entity_type(items)
                    input_entity = mapper[mapper_key](items)
                    logger.debug(
                        "------------initiating inputs payload delivery to Atlas-------------"
                    )
                    if INTERACT_WITH_ATLAS == "TRUE":
                        if USE_KAFKA_CONN == "FALSE":
                            input_entity.push_to_atlas_api(input_entity.get_json())
                        else:
                            input_entity.push_to_atlas_kafka_topic(
                                input_entity.get_json_kafka_version()
                            )
                    if INTERACT_WITH_OPENMETADATA == "TRUE":
                        '''TODO: Upcoming version interact and send lineage to Open Metadata'''
                        pass 
                else:
                    logger.debug(
                        "rejected message for parsing due to missing required key called FACETS in inputs!! "
                    )
                    logger.debug(f"rejected message: {facet}")
        if "outputs" in facet and len(facet["outputs"]) > 0:
            for items in facet["outputs"]:
                if "facets" in items:
                    mapper_key = identify_entity_type(items)
                    output_entity = mapper[mapper_key](items)
                    logger.debug(
                        "------------initiating outputs payload delivery to Atlas-------------"
                    )
                    if INTERACT_WITH_ATLAS == "TRUE":
                        if USE_KAFKA_CONN == "FALSE":
                            output_entity.push_to_atlas_api(output_entity.get_json())
                        else:
                            output_entity.push_to_atlas_kafka_topic(
                                output_entity.get_json_kafka_version()
                            )
                    if INTERACT_WITH_OPENMETADATA == "TRUE":
                        '''TODO: Upcoming version interact and send lineage to Open Metadata'''
                        pass
                else:
                    logger.debug(
                        "rejected message for parsing due to missing required key called FACETS in outputs!! "
                    )
                    logger.debug(f"rejected message:{facet}")
        else:
            logger.debug(
                "Null inputs or outputs facets are detected in the message!! Skipping Spark_process lineage creation"
            )
    else:
        pass
'''
This method if invoked, 
-> Will accepts a message from kafka topic,

-> Decodes the message from utf-8

-> Deserializes message into list of messages by calling "deserialize method".

Note: In many instances, open lineage spark emitter will emit multiple event messages
not separated by ",". 
"deserialize method will make it more organized for parsing.

-> And iterates over the list of deserialized messages by passing each message
to "transformer" method

-> Additionally, it also logs the KPIs such as message size in Kb,
start time of processing and end time of processing. These metrics are by default
 and will be logged at "info" level. 
 
'''
def parser(message):
    logger.debug("\n Consumer initiated !!")
    try:
        decoded_message = message.value().decode("utf-8")
        logger.debug(
            f"Connected to Topic: {message.topic()} and Partition: {message.partition()}"
        )
        logger.debug(
            f"Received Message: {decoded_message} with Offset: {message.offset()}"
        )
        message_size = len(decoded_message.encode('utf-8'))/1024
        processing_start_time = datetime.datetime.now()
        logger.debug("\n deserializing the message!!")
        deserialized_message = deserialize(decoded_message)
        logger.debug("\n converting")
        deserialized_data_list = json.loads(deserialized_message)
        for facet in deserialized_data_list:
            try:
                transform(facet)
                processing_end_time = datetime.datetime.now()
                elapsed_time = processing_end_time - processing_start_time
                logger.info(f'size={message_size}kb,start={processing_start_time},end={processing_end_time},elapsed={elapsed_time.seconds}')
            except Exception as e:
                logger.error(str(e))
                continue
    except Exception as e:
        logger.error(str(e))

'''
Below is the main initiation which has the consumer configs,
and this initiates the above parser method there by triggering entire flow.
Currently, spark lineage transform can only receive event messages
from open lineage spark listener through kafka topic.
The API capability will soon be implemented after the initial release
'''
if __name__ == "__main__":
    consumer_cofig_dict = {
        "bootstrap.servers": retrieve_env_var("KAFKA_BOOTSTRAP_SERVERS"),
        "group.id": retrieve_env_var("KAFKA_GROUP_ID"),
        "security.protocol": retrieve_env_var("KAFKA_SECURITY_PROTOCOL"),
        "auto.offset.reset": retrieve_env_var("KAFKA_AUTO_OFFSET_RESET"),
        "enable.auto.commit": True,
        "heartbeat.interval.ms": 50000,
        "session.timeout.ms": 300000,
        "max.poll.interval.ms": 700000,
        "sasl.mechanisms": retrieve_env_var("KAFKA_SASL_MECHANISMS"),
        "sasl.username": retrieve_env_var("KAFKA_SASL_USERNAME"),
        "sasl.password": retrieve_env_var("KAFKA_SASL_PASSWORD"),
        "ssl.ca.location": retrieve_env_var("KAFKA_SSL_CA_LOCATION"),
    }
    OPENLINEAGE_KAFKA_TOPIC_NAME = retrieve_env_var("OPEN_LINEAGE_KAFKA_TOPIC_NAME")
    consumer = Consumer(consumer_cofig_dict)
    logger.info("consumer configured successfully!")
    consumer.subscribe([OPENLINEAGE_KAFKA_TOPIC_NAME])
    logger.info(
        f"consumer subscribed to topic {OPENLINEAGE_KAFKA_TOPIC_NAME} successfully!"
    )
    while True:
        READ_FROM_KAFKA = retrieve_env_var("SPARK_LINEAGE_TRANSFORM_READ_FROM_KAFKA")
        if READ_FROM_KAFKA == "TRUE":
            message = consumer.poll(1.0)
            current_date = datetime.datetime.now()
            log_time = current_date.strftime("%B %d time- %H : %M : %S")
            if message is not None:
                logger.info(parser(message))
            elif message is None:
                logger.debug(
                    f" no messages detected in topic {OPENLINEAGE_KAFKA_TOPIC_NAME} at {log_time}!"
                )
                logger.debug(" waiting for 1 sec to try again!! ")
                time.sleep(1.0)
                continue
        else:
            '''TODO: future version, add API support'''
